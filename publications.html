<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Title -->
	<title>Yihan Wang's Homepage</title>
	
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Monda">
	<link rel="stylesheet" href="static/custom-navbar.css">
    <link rel="stylesheet" href="static/styles.css">

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131374046-3"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-131374046-3');
	</script>

	<!-- Show more content -->
	<script type="text/javascript">
		function display(id) {
            var currentDisplay = document.getElementById(id).style.display;
            if (currentDisplay == 'none') {
                document.getElementById(id).style.display = "inline-block";
            }
            else {
                document.getElementById(id).style.display = "none";
            }
        }
	</script>

	<!-- Github button icon -->
	<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>


<body>
	<nav class="navbar" role="navigation">
        <div class="navbar-brand">
          <a class="navbar-item" href="/">
            <strong>Yihan Wang</strong>
          </a>
  
          <div class="navbar-burger" data-target="navbar-main">
            <span></span>
            <span></span>
            <span></span>
          </div>
        </div>
  
        <div class="navbar-menu" id="navbar-main">
            <div class="navbar-start">
                <!-- navbar items -->
                <a href="/" class="navbar-item">
                About
                </a>
                <a class="navbar-item" href="publications.html">
                Publications
                </a>
                <a class="navbar-item" href="experience.html">
                Experience
                </a>
            </div>
        </div>
    </nav>


    <!-- Publications -->
	<div class="container" style="font-size: 16px">
		<h2 id="Publications" style="padding-top: 90px; margin-top: -80px; margin-bottom: 0.1em">Publications </h2>
		<font style="font-size: 16px;" color="black">(* indicates equal contribution)</font>
		<hr style="margin-top:0.4em">

		<div class="row">
			<div class="col-md-3">
				<img class="img-thumbnail" src="static/infinigen.jpeg" alt="">
			</div>
			<div class="col-md-9">
				<font style="font-size: 18px;" color="black"><strong>Infinite Photorealistic Worlds using Procedural Generation<!-- Place this tag where you want the button to render. -->
					<a class="github-button" href="https://github.com/princeton-vl/infinigen" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star princeton-vl/infinigen on GitHub">Star</a></strong><br></font>
				<span>Alexander Raistrick*</span>,
				<span>Lahav Lipson*</span>,
				<span>Zeyu Ma*</span>,
				<span>Lingjie Mei</span>,
				<span>Mingzhe Wang</span>,
				<span>Yiming Zuo</span>,
				<span>Karhan Kayan</span>,
				<span>Hongyu Wen</span>,
				<span>Beining Han</span>,
				<span><u>Yihan Wang</u></span>,
				<span>Alejandro Newell</span>,
				<span>Hei Law</span>,
				<span>Ankit Goyal</span>,
				<span>Kaiyu Yang</span>,
				<span>Jia Deng</span>
				<br>
				<i><font color="black"> Conference on Computer Vision and Pattern Recognition </i> (CVPR), 2023 </font>
				<br>
				[<a href="javascript:void(0)" onclick="display('infinigen-abs')">abs</a>]&nbsp;
				[<a target="_blank" href="https://arxiv.org/abs/2306.09310">arXiv</a>]&nbsp;
				[<a target="_blank" href="https://github.com/princeton-vl/infinigen">code</a>]&nbsp;
				[<a target="_blank" href="https://infinigen.org/">website</a>]&nbsp;
				[<a target="_blank" href="https://youtu.be/6tgspeI-GHY">video</a>]
				<br>
				<div id="infinigen-abs" style="display:none">
					<p>
						We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely
						procedural: every asset, from shape to texture, is generated
						from scratch via randomized mathematical rules, using no
						external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes
						in the natural world including plants, animals, terrains, and
						natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training
						data for a wide range of computer vision tasks including
						object detection, semantic segmentation, optical flow, and
						3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond.
 
					</p>
				</div>
			</div>
		</div>

		<hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-thumbnail" src="static/litepose.png" alt="">
			</div>
			<div class="col-md-9">
				<font style="font-size: 18px;" color="black"><strong>Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation<!-- Place this tag where you want the button to render. -->
					<a class="github-button" href="https://github.com/mit-han-lab/litepose" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star mit-han-lab/litepose on GitHub">Star</a></strong><br></font>
				<span><u>Yihan Wang</u></span>,
				<span>Muyang Li</span>,
				<span>Han Cai</span>,
				<span>Wei-Ming Chen</span>,
				<span>Song Han</span>
				<br>
				<i><font color="black"> Conference on Computer Vision and Pattern Recognition </i> (CVPR), 2022 </font>
				<br>
				[<a href="javascript:void(0)" onclick="display('litepose-abs')">abs</a>]&nbsp;
				[<a target="_blank" href="https://arxiv.org/abs/2205.01271">arXiv</a>]&nbsp;
				[<a target="_blank" href="https://github.com/mit-han-lab/litepose">code</a>]&nbsp;
				[<a target="_blank" href="https://litepose.mit.edu/">website</a>]&nbsp;
				[<a target="_blank" href="https://www.youtube.com/watch?v=TodvXYrswDI">video</a>]
				<br>
				<div id="litepose-abs" style="display:none">
					<p>
						Pose estimation plays a critical role in human-centered
						vision applications. However, it is difficult to deploy state-ofthe-art HRNet-based pose estimation models [8] on resourceconstrained edge devices due to the high computational
						cost (more than 150 GMACs per frame). In this paper,
						we study efficient architecture design for real-time multiperson pose estimation on edge. We reveal that HRNet's
						high-resolution branches are redundant for models at the
						low-computation region via our gradual shrinking experiments. Removing them improves both efficiency and performance. Inspired by this finding, we design LitePose,
						an efficient single-branch architecture for pose estimation,
						and introduce two simple approaches to enhance the capacity of LitePose, including Fusion Deconv Head and Large
						Kernel Convs. Fusion Deconv Head removes the redundancy in high-resolution branches, allowing scale-aware
						feature fusion with low overhead. Large Kernel Convs significantly improve the model's capacity and receptive field
						while maintaining a low computational cost. With only 25%
						computation increment, 7x7 kernels achieve +14.0 mAP
						better than 3x3 kernels on the CrowdPose dataset. On
						mobile platforms, LitePose reduces the latency by up to
						5.0x without sacrificing performance, compared with prior
						state-of-the-art efficient pose estimation models, pushing
						the frontier of real-time multi-person pose estimation on
						edge.
					</p>
				</div>
			</div>
		</div>

		<hr>

		<div class="row">
			<div class="col-md-3">
				<img class="img-thumbnail" src="static/dop.png" alt="">
			</div>
			<div class="col-md-9">
				<font style="font-size: 18px;" color="black"><strong>DOP: Off-Policy Multi-Agent Decomposed Policy Gradients<!-- Place this tag where you want the button to render. -->
					<a class="github-button" href="https://github.com/TonghanWang/DOP" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star TonghanWang/DOP on GitHub">Star</a></strong><br></font>
				<span><u>Yihan Wang</u>*</span>,
				<span>Beining Han*</span>,
				<span>Tonghan Wang*</span>,
				<span>Heng Dong</span>,
				<span>Chongjie Zhang</span>
				<br>
				<i><font color="black"> International Conference on Learning Representations</i> (ICLR), 2021 </font>
				<br> 
				[<a href="javascript:void(0)" onclick="display('dop-abs')">abs</a>]&nbsp;
				[<a target="_blank" href="https://arxiv.org/abs/2007.12322">arXiv</a>]&nbsp;
				[<a target="_blank" href="https://github.com/TonghanWang/DOP">code</a>]&nbsp;
				[<a target="_blank" href="https://sites.google.com/view/dop-mapg/">video</a>]&nbsp;
				<div id="dop-abs" style="display:none">
					<p>
						Multi-agent policy gradient (MAPG) methods recently witness vigorous progress.
						However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we
						investigate causes that hinder the performance of MAPG algorithms and present a
						multi-agent decomposed policy gradient method (DOP). This method introduces the
						idea of value function decomposition into the multi-agent actor-critic framework.
						Based on this idea, DOP supports efficient off-policy learning and addresses the
						issue of centralized-decentralized mismatch and credit assignment in both discrete
						and continuous action spaces. We formally show that DOP critics have sufficient
						representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle
						environments demonstrate that DOP significantly outperforms both state-of-theart value-based and policy-based multi-agent reinforcement learning algorithms.
					</p>
				</div>
			</div>
		</div>
		<br>
		<br>
	</div>


    <!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" crossorigin="anonymous"></script>
	<script src="burger.js"></script>
</body>

</html>